!pip install prettyTable

import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def xor_network(x1, x2, epochs=10000, learning_rate=0.1):
    # Network with 2 hidden neurons and 1 output neuron, sigmoid activation
    W1 = np.random.randn(2, 2)  # Weights for hidden layer
    b1 = np.random.randn(2, 1)  # Biases for hidden layer
    W2 = np.random.randn(1, 2)  # Weights for output layer
    b2 = np.random.randn(1, 1)  # Bias for output layer

    # XOR truth table for training data
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])

    # Training loop for multiple epochs
    for epoch in range(epochs):
        for i in range(X.shape[0]):
            # Forward pass
            z1 = np.dot(W1, X[i].reshape(2, 1)) + b1
            a1 = sigmoid(z1)
            z2 = np.dot(W2, a1) + b2
            a2 = sigmoid(z2)

            # Backpropagation
            dz2 = a2 - y[i]
            dW2 = np.dot(dz2, a1.T)
            db2 = dz2
            dz1 = np.dot(W2.T, dz2) * (a1 * (1 - a1))
            dW1 = np.dot(dz1, X[i].reshape(1, 2))
            db1 = dz1

            # Parameter updates
            W1 -= learning_rate * dW1
            b1 -= learning_rate * db1
            W2 -= learning_rate * dW2
            b2 -= learning_rate * db2

    # Prediction for the input
    z1 = np.dot(W1, np.array([[x1], [x2]])) + b1
    a1 = sigmoid(z1)
    z2 = np.dot(W2, a1) + b2
    a2 = sigmoid(z2)

    return np.round(a2[0][0])

from prettytable import PrettyTable
table = [['A', 'B', 'XOR'], [0, 0, xor_network(0, 0)], [1, 0, xor_network(1, 0)], [0, 1, xor_network(0, 1)], [1, 1, xor_network(1, 1)]]
tab = PrettyTable(table[0])
tab.add_rows(table[1:])
print(tab)
